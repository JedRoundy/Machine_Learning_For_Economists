{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JedRoundy/Machine_Learning_For_Economists/blob/main/PSET_5/pset5coding_TEMPLATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8EG0JSF7pnq"
      },
      "outputs": [],
      "source": [
        "# import the modules and function you will use here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_abhzZe7pns"
      },
      "source": [
        "This problem deals with regularized regression. The boston dataset is described right after it is loaded in just by running the code that is aleardy there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u--QcpA7pnt"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "print(boston['DESCR'])\n",
        "x = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
        "y = pd.Series(boston['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKZLn6jq7pnt"
      },
      "source": [
        "$(a)$ Split the data into a train and a test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k4PTOA17pnu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ1Tdu3Y7pnu"
      },
      "source": [
        "$(b)$ Use this data to fit an OLS, LASSO, ridge, and ElasticNet model on the data. For now, use the default for the penalty coefficient. Display the coefficients and test error for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d_HP88e7pnv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaJ8PFR_7pnv"
      },
      "source": [
        "$(c)$ Describe the differences that you see in the coefficients and error. What is the cause of this difference in coefficients?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bx_zNuQ7pnv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFG4_Jre7pnx"
      },
      "source": [
        "$(d)$ Use K-fold cross validation to find an optimal penalty parameter for Ridge and Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsTTo6Uv7pnx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zsVqtUH7pnx"
      },
      "source": [
        "$(e)$ Now use cross validation, to find the optimal penalty parameter. Use LOOCV and Kfold cross validation with K=5 to find optimal parameters for the ElasticNet model. How do the test errors and optimal parameters differ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erKkFgqN7pnx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrgSWUz7pnx"
      },
      "source": [
        "$(f)$ Now that we have tuned the models to perform about as well as they can, which one performs best on the training data? Which one performs best on the test data? Which of these models allow us to do effective causal inference with the coefficients? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbKaNzj07pny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wz8O0bi7pny"
      },
      "source": [
        "For the next problem we will be using the `Carseats` data set that is available on learningsuite. Load the data and convert the text variables into dummies so that we can use them in the data. Pandas has a function called `get_dummies` that you might want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7an3ikpW7pny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw2vziiz7pny"
      },
      "source": [
        "Now that the data has only numeric columns, we can proceed to the analysis.  \n",
        "Use `Sales` as the outcome variable  \n",
        "(a) Split the data set into a training set and a test set.  \n",
        "(b) Fit a regression tree to the training set with the default depth. What train and test MSE do you obtain?  \n",
        "(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE? Plot a tree with a depth of 3, and interpret the results.  \n",
        "(d) Use a bagging approach in order to analyze this data. What test MSE do you obtain? Look at the feature importances attribute of your model object to determine which variables are most important.  \n",
        "(e) Use random forests to analyze this data. What test MSE do you obtain? Look at the feature importances attribute of your model object function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRTwVupn7pnz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UUhmvR7pnz"
      },
      "source": [
        "We will now use boosting to predict Log Salary in the `Hitters` data set.  \n",
        "(a) Format the data appropriately for this analysis. Use 200 observations in your training set.  \n",
        "(b) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter Î». Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis. Add a curve with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis. The shrinkage parameter is often referred to as the learning rate   \n",
        "(c) Compare the test MSE of boosting to the test MSE of two of the penalized regression approaches that we discussed  \n",
        "(d) Which variables appear to be the most important predictors in the boosted model?  \n",
        "(e) The default for base estimator is a Decision Tree with a maximum depth of 3. Is that the optimal depth? Justify your response.  \n",
        "(f) Now that the boosting model is tuned, let's compare the results to bagging and random forests. Report test errors for your models and discuss how they compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JAb327T7pnz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHleB2iC7pn0"
      },
      "source": [
        "In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.  \n",
        "\n",
        "#### NOTE: SVM algortihms will often take longer than other models to train, particularly when doing cross validation\n",
        "\n",
        "(a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.  \n",
        "(b) Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.  \n",
        "(c) Make an ROC curve for your model. The module scikitplot has a nice function you might want to use but you should eb able to make it on your own or another module if you desire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n5JN4gT7pn0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ63Cdcg7pn1"
      },
      "source": [
        "Below there are some generated datasets of varying structure that you will classifying is SVMs, plotting the data to see what it looks like will likey be helpful. Find the basis kernel that does best job classifying each of them. Because the data is two dimensional, it might be nice to use a library like mlxtend which has a function that will display decision regions form an svm using a one of their functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKTlWxD87pn1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "x, y = make_moons(n_samples=100, shuffle=True, noise=1/10, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLav1_DV7pn1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "x, y = make_circles(n_samples=100, shuffle=False, noise=1/50, random_state=123, factor=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN5Tfwdv7pn1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "x, y = make_blobs(n_samples=100, n_features=2, centers=None, cluster_std=2.0,\n",
        "           center_box=(-10.0, 10.0), shuffle=True, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1m0ORXS7pn2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}