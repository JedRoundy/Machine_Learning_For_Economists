{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JedRoundy/Machine_Learning_For_Economists/blob/main/midterm/midterm2023fall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzFDXTTnWM9c"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed=484)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JYG5qnnWM9f"
      },
      "source": [
        "# Midterm Exam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiFzYF2WM9g"
      },
      "source": [
        "Exam is open book, open note, and open Google. You are not allowed outside\n",
        "help from another person, however. All work, including coding, must be yours alone. Remember to turn in both the written portion and this coding portion. The coding portion can be turned in by submitting a shared link to your Colab notebook. To complete this coding portion, make sure to save a copy of this notebook in your own Google drive, supply the python code in the empty cells below, and execute the notebook. To get full credit, the completed notebook should be able to run top to bottom, producing the results asked for in the prompts below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkCjCsX9WM9g"
      },
      "source": [
        "This portion of the exam will take you through the steps of the supervised machine learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UamL0RdiWM9h"
      },
      "source": [
        "## 1. Figure out your question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ7P55DJWM9h"
      },
      "source": [
        "The question you want to answer is: How does childbearing impact labor market outcomes for women? We can use machine learning to help answer this question by building a model that predicts how many children a woman gives birth to on the basis of her characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6zoctZZWM9h"
      },
      "source": [
        "## 2. Obtain a labeled dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdgfGHzYWM9i"
      },
      "source": [
        "Import the python library that is good for manipulating datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3wmGngUWM9i"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwWVnmc_WM9k"
      },
      "source": [
        "Accompanying the exam materials are a spreadsheet of female survey respondents, 'femalelaborsupply.csv' and a text file, 'femalelaborsupplydefs.txt' that explains each variable in the spreadsheet. Read in the data in the spreadsheet 'femalelaborsupply.csv', print out the first few rows of data with the variable names, and print out the number of observations and variables in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fer02-GdWM9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36be4e9-54af-40aa-da08-e424a47042d3"
      },
      "source": [
        "fls = pd.read_csv('https://www.dropbox.com/s/r5ahpsb6kt63fw3/femalelaborsupply.csv?dl=1')\n",
        "\n",
        "print(fls.head())\n",
        "print('\\n')\n",
        "print(f'Number of observations: {len(fls)}')\n",
        "print(f'Number of variables in dataset: {len(fls.columns)}')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   asex  aage  aqtrbrth  ageqk  asex2nd  aage2nd  ageq2nd  ageq3rd  kidcount  \\\n",
            "0     0     0         0     36        0        0       30      NaN         2   \n",
            "1     0     0         0     23        0        0        9      NaN         2   \n",
            "2     0     0         0     44        0        0       22      NaN         2   \n",
            "3     0     0         0     24        0        0       12      NaN         2   \n",
            "4     0     0         0     28        0        0       14      NaN         2   \n",
            "\n",
            "   agem  ...  hourswm    incomed    incomem    faminc1    famincl     nonmomi  \\\n",
            "0    27  ...        0  33597.273      0.000  33597.273  10.422200  33597.2730   \n",
            "1    25  ...       38        NaN  18273.307  21642.479   9.982413   3369.1726   \n",
            "2    30  ...       40  20834.297  18903.059  43326.941  10.676530  24423.8830   \n",
            "3    27  ...        0  30658.430      0.000  30658.430  10.330663  30658.4300   \n",
            "4    35  ...        0  44450.000      0.000  44450.000  10.702120  44450.0000   \n",
            "\n",
            "    nonmomil  qobm  const  msample  \n",
            "0  10.422200     2      1        1  \n",
            "1   8.122422     3      1        0  \n",
            "2  10.103316     4      1        1  \n",
            "3  10.330663     3      1        1  \n",
            "4  10.702120     1      1        1  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "\n",
            "\n",
            "Number of observations: 394840\n",
            "Number of variables in dataset: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQnjWhASWM9m"
      },
      "source": [
        "Define a label (outcome) vector, $y_1$, to be how many children the woman has, another outcome vector, $y_2$ to be an indicator for having three or more children, and define a feature (regressor) matrix, $X$, to contain the mother's age, marital status, race, ethnicity, and education:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMpxIqiHWM9m",
        "outputId": "ad8572af-ce8c-40af-efa1-46968b63f360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y1 = fls['kidcount']\n",
        "y2 = fls['morekids']\n",
        "X = fls[['agem1', 'marital', 'blackm', 'hispm', 'othracem', 'educm']]\n",
        "\n",
        "\n",
        "X['Married w/ Spouse'] = [1 if x == 0 else 0 for x in X['marital']]\n",
        "X['Married w/o Spouse'] = [1 if x == 1 else 0 for x in X['marital']]\n",
        "X['Separated'] = [1 if x == 2 else 0 for x in X['marital']]\n",
        "X['Divorced'] = [1 if x == 3 else 0 for x in X['marital']]\n",
        "X['Widowed'] = [1 if x == 4 else 0 for x in X['marital']]\n",
        "\n",
        "\n",
        "len(X['marital'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "394840"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OEvMdp-Xj6m"
      },
      "source": [
        "\"Pre-process\" your features, $X$, by standardizing them to have zero mean and unit variance. Hint: you may import a useful package to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNiZolJcXv6C"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSuEbRMKWM9o"
      },
      "source": [
        "## 3. Divide into training and set sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi-dbFQyWM9o"
      },
      "source": [
        "Import the python library that is good for randomly splitting datasets into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpTtWuWsWM9p"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5tGzFUWM9r"
      },
      "source": [
        "Now make a training and test feature matrix and a training and test label vectors $y_1$ and $y_2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im4z_YPAWM9r"
      },
      "source": [
        "X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, random_state = 0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzVMOGgrWM9t"
      },
      "source": [
        "## 4. Pick an appropriate method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbrMr8KFWM9t"
      },
      "source": [
        "Choose a method appropriate for classification and import its library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0rbHexoWM9u"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0muOgJWM9y"
      },
      "source": [
        "## 5 and 6. Choose regularization parameters via cross-validation on the training set and fit model on the whole training set using the cross-validated parameters\n",
        "\n",
        "The outcome you should use in this part is $y_2$, the indicator for having at least three kids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT5ASK5pWM9y"
      },
      "source": [
        "Search over a grid of values of the regularization parameters for the parameters that perform the best on the left-out folds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Cm_rNVWM9y",
        "outputId": "e881bb4e-3d6c-4f4b-f9f6-c305d61d910f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {'n_estimators': [100, 200, 300], 'max_depth': [6,7,8], 'min_samples_split': [10, 20, 30]}\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "model = GridSearchCV(estimator = rfc, param_grid = params, cv = 4, verbose = 2)\n",
        "\n",
        "model.fit(X_train, y2_train)\n",
        "\n",
        "model.score(X_test, y2_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=  11.4s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   6.0s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   7.1s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   6.4s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=  13.5s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=  13.7s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=  13.4s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=  13.0s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=300; total time=  19.8s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=300; total time=  20.4s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=300; total time=  20.2s\n",
            "[CV] END max_depth=6, min_samples_split=10, n_estimators=300; total time=  19.6s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=100; total time=   7.1s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=100; total time=   6.1s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=100; total time=   6.1s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=200; total time=  12.9s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=200; total time=  13.3s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=200; total time=  13.1s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=300; total time=  18.7s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=300; total time=  19.8s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=300; total time=  19.9s\n",
            "[CV] END max_depth=6, min_samples_split=20, n_estimators=300; total time=  19.9s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=100; total time=   6.0s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=100; total time=   6.8s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=100; total time=   6.4s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=100; total time=   6.6s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=200; total time=  12.9s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=200; total time=  13.0s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=200; total time=  12.8s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=200; total time=  12.9s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=300; total time=  19.6s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=300; total time=  18.8s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=300; total time=  19.7s\n",
            "[CV] END max_depth=6, min_samples_split=30, n_estimators=300; total time=  18.8s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=100; total time=   7.4s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=100; total time=   6.4s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=100; total time=   8.4s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=200; total time=  13.7s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=200; total time=  13.7s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=200; total time=  13.8s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=300; total time=  21.0s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=300; total time=  20.1s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=300; total time=  20.9s\n",
            "[CV] END max_depth=7, min_samples_split=10, n_estimators=300; total time=  20.9s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=100; total time=   6.5s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=100; total time=   6.9s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=100; total time=   7.1s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=200; total time=  13.8s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=200; total time=  14.9s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=200; total time=  14.4s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=300; total time=  19.9s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=300; total time=  20.8s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=300; total time=  20.9s\n",
            "[CV] END max_depth=7, min_samples_split=20, n_estimators=300; total time=  20.0s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=100; total time=   6.4s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=100; total time=   7.5s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=100; total time=   7.0s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=200; total time=  13.9s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=200; total time=  13.7s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=300; total time=  20.8s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=300; total time=  20.7s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=300; total time=  21.2s\n",
            "[CV] END max_depth=7, min_samples_split=30, n_estimators=300; total time=  21.1s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   6.9s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   7.7s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=  14.3s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=  16.7s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=  14.7s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=  14.5s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=300; total time=  22.4s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=300; total time=  21.7s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=300; total time=  22.4s\n",
            "[CV] END max_depth=8, min_samples_split=10, n_estimators=300; total time=  22.2s\n",
            "[CV] END max_depth=8, min_samples_split=20, n_estimators=100; total time=   7.6s\n",
            "[CV] END max_depth=8, min_samples_split=20, n_estimators=100; total time=   6.7s\n",
            "[CV] END max_depth=8, min_samples_split=20, n_estimators=100; total time=   7.7s\n",
            "[CV] END max_depth=8, min_samples_split=20, n_estimators=100; total time=   7.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qg1nZnpWM92"
      },
      "source": [
        "## 7. Evaluate model by applying it to test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA7l42zLWM92"
      },
      "source": [
        "Compute and print out the \"score\" of the model applied to the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-LocbNpWM92"
      },
      "source": [
        "model.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2fXVbgRWM94"
      },
      "source": [
        "## 8. Repeat 4-7 for $y_1$\n",
        "using a method appropriate for regression-style prediction to predict number of children, not the probability of having at least three children"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yHi6UvEWM95"
      },
      "source": [
        "Import the method's library, do cross validation to find tuning parameters, fit the model on the training data using the cross-validated tuning parameters, and compute (and report) the model's score on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQbfbPeqWM95"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "params = {'n_estimators': [100, 500, 1000, 2000], 'max_depth': [3,4,5,6,7,8], 'min_samples_split': [10, 20, 30]}\n",
        "\n",
        "rfr = RandomForestRegressor()\n",
        "\n",
        "model = GridSearchCV(estimator = rfr, param_grid = params, cv = 4)\n",
        "\n",
        "model.fit(X_train, y1_train)\n",
        "\n",
        "model.score(X_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhyyawznWM96"
      },
      "source": [
        "## 9. Apply the prediction  models to new observations for which we have no labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzPB_fH7WM99"
      },
      "source": [
        "The spreadsheet 'newfemales.csv' contains information on two new females, with identical characteristics, except one is a high school graduate, and the other has a bachelor's degree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvZIvvAbWM99"
      },
      "source": [
        "Read in the new observations' information and apply the models to predict the probability of each applicant having at least three kids, and the predicted number of kids each applicant will have, and print out the predictions. Hint: don't forget to apply the same pre-processing steps to the new observations as you did to your training and test observations. This means standardizing the new observations using the means and variances of your labeled dataset, not the means and variances of these two new observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": false,
        "id": "6EV3sNlRWM99"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}